# Optimal Sample Size for Multiple Discoveries
**Stochastic Target Formulation for Sequential Multiple Testing**

This repository accompanies an ongoing research project on **optimal sample size** and **optimal allocation of sampling effort** in a **continuous-time multiple testing** setting.

The central question is:

> How much total sampling time do we need, and how should we allocate it across arms, in order to  
> (i) control the Family-Wise Error Rate (FWER) at level \(\alpha\),  
> (ii) guarantee a Family-Wise Probability of Detection (FWPD) at least \(1-\beta\),  
> while minimizing the total sampling horizon \(T\)?

The project is a **math-stat inquiry**, not a software package. All proofs and technical details are in the companion paper (Overleaf link).
This README gives a higher-level description of the model and main ideas.


## 1. Model in Continuous Time

We work on a filtered probability space
\[
(\Omega,\mathcal{F},\mathbb{F}=(\mathcal{F}_t)_{t\ge 0},\mathbb{P}),
\]
supporting \(N\) independent standard Brownian motions \(W^1,\dots,W^N\).

A **sampling allocation policy** is an \(\mathbb{F}\)-adapted process
\[
\mathbf{a}_t = (a_t^1,\dots,a_t^N) \in \Delta_N,
\]
where \(\Delta_N\) is the unit simplex, and
\[
A_t^i = \int_0^t a_s^i\,ds
\]
is the accumulated sampling time on arm \(i\).

Conditionally on a fixed allocation \(\mathbf{a}\), the **evidence process** on arm \(i\) is
\[
dX_t^{i,a_i} = \theta^i a_t^i\,dt + \sigma^i \sqrt{a_t^i}\,dW_t^i,\qquad X_0^{i,a_i}=0.
\]

We consider simple hypotheses for each arm:
\[
H_0^i:\ \theta^i = \theta_0^i
\quad\text{vs.}\quad
H_1^i:\ \theta^i = \theta_1^i = \theta_0^i + \Delta^i,
\]
and for most of the analysis we adopt the homogeneous case
\[
\sigma^i = \sigma,\quad\theta_0^i = \theta_0,\quad\theta_1^i = \theta_1 = \theta_0 + \Delta
\quad\text{for all } i.
\]

The key objects are the **likelihood ratio** and the associated **e-process** for each arm.


## 2. Likelihood Ratio and E-Processes

For arm \(i\), the log-likelihood ratio for testing \(H_0^i\) vs \(H_1^i\) is
\[
L_t^i
=
\int_0^t \frac{\Delta\,a_s^i}{\sigma^2}\,dX_s^i
- \frac{1}{2}\int_0^t \frac{\Delta^2 (a_s^i)^2}{\sigma^2}\,ds.
\]

We then define the **e-process**
\[
E_t^i = \exp(L_t^i),\qquad t\ge 0.
\]

Under the null (i.e. under a measure \(\mathbb{P}_0^i\) with \(\theta^i=\theta_0\)),
\[
dE_t^i
=
E_t^i \,\frac{\Delta}{\sigma}\, a_t^i\, dW_t^i,
\]
so \((E_t^i)_{t\ge 0}\) is a **nonnegative \(\mathbb{P}_0^i\)-martingale**. This is the core martingale property used for anytime-valid testing with e-values.

We collect the e-processes into
\[
\mathbf{E}_t = (E_t^1,\dots,E_t^N).
\]

Importantly, the filtration generated by \(\mathbf{E}\) is the same as the one generated by \(\mathbf{X}\):
\[
\sigma(\mathbf{E}_s, 0\le s\le t)
=
\sigma(\mathbf{X}_s, 0\le s\le t),
\]
so we can work with \(\mathbf{E}\) as the state variable of the control problem.


## 3. Multiple Testing: FWER and FWPD

Let \(\mathcal{H}_0\) denote the set of indices with true nulls and \(\mathcal{H}_1\) the set of indices with true alternatives under the (unknown) true law \(\mathbb{P}\).

At any time \(t\), a **discovery set** \(S_t \subseteq \{1,\dots,N\}\) collects the indices of arms where \(H_0^i\) is rejected.

### 3.1 FWER-\(\alpha\) (Anytime Control)

A procedure controls the **Family-Wise Error Rate** at level \(\alpha\in(0,1)\) if, under the global null \(\mathbb{P}_0\),
\[
\mathbb{P}_0\big( \exists t\ge 0: S_t \cap \mathcal{H}_0 \neq \emptyset \big) \le \alpha.
\]

Using e-processes, a simple FWER-\(\alpha\) rule is:
\[
S_t^{\mathrm{FWER}} = \{ i : E_t^i \ge 1/\alpha \}.
\]

By Ville’s inequality and independence under \(\mathbb{P}_0\), this rule controls the FWER at level \(\alpha\) in an anytime sense.


### 3.2 FWPD-\((1-\beta)\) (Family-Wise Probability of Detection)

We measure the ability to detect all truly non-null arms through the **Family-Wise Probability of Detection**:
\[
\mathrm{FWPD}(t) = \mathbb{P}\big( \mathcal{H}_1 \subseteq S_t \big).
\]

For a fixed horizon \(T\), we say that a procedure achieves **FWPD-\((1-\beta)\) by time \(T\)** if
\[
\mathbb{P}\big( \mathcal{H}_1 \subseteq S_T \big) \ge 1-\beta.
\]

Under the FWER rule \(S_T^{\mathrm{FWER}}\), this condition is equivalent to
\[
\mathbb{P}\Big( \min_{i \in \mathcal{H}_1} E_T^i \ge 1/\alpha \Big) \ge 1-\beta.
\]

This is our **target event** for detection.


## 4. Stochastic Target Formulation

We work under a reference alternative law \(\mathbb{P}_1\) where all arms in \(\mathcal{H}_1\) follow the alternative parameter.

We introduce the target event
\[
\mathcal{E}^\alpha(\mathbf{E}_T)
=
\left\{
\min_{i\in\mathcal{H}_1} E_T^i \ge 1/\alpha
\right\}.
\]

Given a starting time \(t\) and state \(\mathbf{E}_t = e\), we define the **value function**
\[
V(t,e) = 
\inf
\left\{
T \ge t \ :\ \exists \mathbf{a} \in \mathcal{A}_t(T) \ \text{s.t.}\
\mathbb{P}_1\big( \mathcal{E}^\alpha(\mathbf{E}_T^{\mathbf{a}}) \big) \ge 1-\beta
\right\},
\]
where \(\mathcal{A}_t(T)\) is the set of admissible allocation strategies on \([t,T]\).

So:

- The **control** is the allocation process \(\mathbf{a}\).
- The **state** is the vector of e-values \(\mathbf{E}_t\).
- The **objective** is to minimize the **horizon** \(T\) subject to a probabilistic target constraint.
- The **target set** is the set of states where all alternative arms have crossed the detection threshold.

This is a **stochastic target problem** with a constraint in probability.  
We follow the framework of Soner–Touzi–Zhang and Bouchard–Vu, adapted to our multi-armed, backward formulation.


## 5. Dynamic Programming and HJB Equation

Using the standard machinery for stochastic target problems with probability constraints, we obtain:

- A **weak dynamic programming principle** for \(V(t,e)\).
- A characterization of \(V\) as a (viscosity) solution of a **Hamilton–Jacobi–Bellman (HJB)** equation.

In short, the dynamics of each e-process under the alternative are of the form
\[
dE_t^i = E_t^i \frac{\Delta}{\sigma} a_t^i dW_t^i + \text{(drift under }\mathbb{P}_1),
\]
and the corresponding generator acting on a smooth test function \(\varphi\) involves terms like
\[
\mathcal{L}^{\mathbf{a}} \varphi(t,e)
=
\frac{1}{2}
\sum_{i=1}^N
\left( \frac{\Delta}{\sigma} \right)^2
(a^i)^2 e_i^2 \,\partial_{e_i e_i}^2 \varphi(t,e)
+ \text{(drift part)}.
\]

The HJB equation has the qualitative structure
\[
\partial_t V(t,e)
+ \inf_{\mathbf{a} \in \Delta_N}
\Big\{ \mathcal{L}^{\mathbf{a}} V(t,e) \Big\}
= 0,
\]
with a terminal/boundary condition encoding the detection target.

The exact analytic form depends on the chosen parametrization (e.g. target in probability vs. target in expectation, augmented states, etc.), and is fully detailed in the paper.

A key qualitative result is that optimal allocations are of **bang–bang** type: at each time, it is optimal to concentrate all sampling effort on a single arm, determined by the local curvature of \(V\).


## 6. Relation to Existing Literature

This work connects several strands of literature:

- **Continuous-time bandits / dynamic allocation**  
  Mandelbaum (1987) and El Karoui & Karatzas (1994) formulate continuous-time multi-armed bandits as control problems on multiparameter processes and study Gittins-type strategies.

- **Stochastic target problems**  
  Soner, Touzi, and Zhang introduce stochastic target problems, and Bouchard & Vu extend them to targets in expectation and probability, providing the dynamic programming and PDE characterization.

- **Sequential multiple testing and e-values**  
  E-processes and e-values provide a unifying framework for anytime-valid inference and online multiple testing (e.g. Ramdas et al., Xu & Ramdas, Wang & Ramdas), with FWER/FDR control based on martingale arguments rather than fixed-time p-values.

This project applies and combines these ideas to address a specific question:  
**What is the minimal sample size to achieve multiple discoveries under strict FWER control, in a continuous-time Gaussian setting, using e-processes?**


## 7. Repository Content

This repository is primarily a **companion to a math-stat paper**.  
It currently contains:

- Notes and drafts for the model and proofs.  
- Sketches of the stochastic target formulation and dynamic programming arguments.  
- (Possibly) simple simulation scripts for illustrative examples.  
- A link to the Overleaf document with the full mathematical development.

The content will evolve as the research progresses.


## 8. How to Read or Contribute

- Start from this README to understand the high-level structure.  
- Consult the Overleaf / PDF document for the full technical details.  
- Issues and pull requests are welcome for:
  - pointing out connections to related work,
  - suggesting alternative formulations of the control/target problem,
  - proposing numerical experiments or toy examples.


## 9. References (selected)

- Bouchard, B., & Vu, T.-N. (2010). *Stochastic target problems with controlled loss*.  
- El Karoui, N., & Karatzas, I. (1994). *Dynamic allocation problems in continuous time*. The Annals of Applied Probability.  
- Mandelbaum, A. (1987). *Continuous multi-armed bandits and multiparameter processes*.  
- Ramdas, A. et al. (various). *E-values and e-processes for sequential and multiple testing*.  
- Soner, H. M., Touzi, N., & Zhang, J. (2002). *Stochastic target problems, dynamic programming, and viscosity solutions*.  
- Xu, T., & Ramdas, A. (2024). *Online FDR control with e-values*, etc.

(See the paper for a complete and precise bibliography.)


## 10. Citation

If you use or discuss this work, please cite:

> Nguyen-Huu, A. (2025).  
> *Optimal Sample Size for Multiple Discoveries*.  
> Working paper.
